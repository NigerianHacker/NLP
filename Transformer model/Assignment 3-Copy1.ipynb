{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e7877d",
   "metadata": {},
   "source": [
    "# Assignmnet 3 (100 + 5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b6d20",
   "metadata": {},
   "source": [
    "**Name:** <br>\n",
    "**Email:** <br>\n",
    "**Group:** A/B <br>\n",
    "**Hours spend *(optional)* :** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f79f88",
   "metadata": {},
   "source": [
    "### Question 1: Transformer model *(100 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd0c1a",
   "metadata": {},
   "source": [
    "As a Machine Learning engineer at a tech company, you were given a task to develop a machine translation system that translates **English (source) to German (Target)**. You can use existing libraries but the training needs to be done from scratch (usage of pretrained weights is not allowed). You have the freedom to select any dataset for training the model. Use a small subset of data as a validation dataset and report the BLEU score on the validation set. Also, provide a short description of your transformer model architecture, hyperparameters, and training (also provide the training loss curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a6e95c",
   "metadata": {},
   "source": [
    "<h3> Submission </h3>\n",
    "\n",
    "The test set **(test.txt)** will be released one week before the deadline. You should submit the output of your model on the test set separately. Name the output file as **\"first name_last_name_test_result.txt\"**. Each line of the submission file should contain only the translated text of the corresponding sentence from 'test.txt'.\n",
    "\n",
    "The 'first name_last_name_test_result.txt' file will be evaluated by your instructor and the student who could get the best BLEU score will get 5 additional points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4d2da",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "Here are some of the parallel datasets (see Datasets and Resources file):\n",
    "* Europarl Parallel corpus - https://www.statmt.org/europarl/v7/de-en.tgz\n",
    "* News Commentary - https://www.statmt.org/wmt14/training-parallel-nc-v9.tgz (use DE-EN parallel data)\n",
    "* Common Crawl corpus - https://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz (use DE-EN parallel data)\n",
    "\n",
    "You can also use other datasets of your choice. In the above datasets, **'.en'** file has the text in English, and **'.de'** file contains their corresponding German translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f05905b",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "1) You can also consider using a small subset of the dataset if the training dataset is large\n",
    "2) Sometimes you can also get out of memory errors while training, so choose the hyperparameters carefully.\n",
    "3) Your training will be much faster if you use a GPU. If you are using a CPU, it may take several hours or even days. (you can also use Google Colab GPUsÂ for training. link: https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5ab6e-8a9c-4cea-ab1f-4c59681d0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "da2ebeb6-e60d-4016-9daf-848b2b56e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab, d_model):\n",
    "        super(Embedding, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.lut = nn.Embedding(vocab, d_model) # look up table \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f77650f-b30f-457c-be87-380d9fbdfd78",
   "metadata": {},
   "source": [
    "<img src='data/img/multi-head-attention_l1A3G7a.png' style='width: 300px'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "270a2b39-b832-4b4a-8f2f-79578baea274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        # the input consists of queries and keys of dim d_k (head dimension)\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        'linear projection (transformation)'\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False) \n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def scaled_dot_product(self, Q, K, V, mask=None):\n",
    "        'Scaled Dot-Product Attention'\n",
    "        d_k = Q.size()[-1]\n",
    "        score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) # scaled multiplcative compatibility function \n",
    "\n",
    "        attention = F.softmax(score, dim=-1) # probability of attention \n",
    "        \n",
    "        return torch.matmul(attention, V)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        'This method splits the input tensor into multiple heads for parallel processing'\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        'After attention computation, the heads are combined back'\n",
    "        batch_size, _, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "                \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attention = self.scaled_dot_product(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attention))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "80a26a34-eae4-459a-93d7-b8f5f5461448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W_q = nn.Linear(512, 512)\n",
    "# Q = torch.randint(low=1, high=1000, size=(8, 100, 512)).float()\n",
    "# x = W_q(Q)\n",
    "# x.size()\n",
    "# Q.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "40e728bb-40fe-47c1-9997-8f398a0c291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc2(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1a2a5c07-5842-4bbc-a842-7047c2971095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0a930-214c-4190-9425-566e48cef974",
   "metadata": {},
   "source": [
    "<img src='data/img/0*bPKV4ekQr9ZjYkWJ.webp' style='width: 300px'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "2548e0a1-53ec-4d8f-9659-b1d99c0b4fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d81cd-dfd2-4885-826c-cfd2b6229920",
   "metadata": {},
   "source": [
    "<img src='data/img/0*SPZgT4k8GQi37H__.webp' style='width: 300px'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "4189e32d-a8cc-4022-8a52-d55b45cf4e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_out = self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "3d3eee18-d7d9-426b-9770-f2a5b1cc806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        # self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([Encoder(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([Decoder(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.encoder_embedding(src))\n",
    "        tgt_embedded = self.dropout(self.decoder_embedding(tgt))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "54d7e00b-572b-426d-a69e-a7a0d5fc2724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Step 1: Load the data from files\n",
    "def load_data(english_file, german_file):\n",
    "    with open(english_file, 'r', encoding='utf-8') as f:\n",
    "        english_sentences = f.readlines()\n",
    "    with open(german_file, 'r', encoding='utf-8') as f:\n",
    "        german_sentences = f.readlines()\n",
    "    return english_sentences, german_sentences\n",
    "    \n",
    "# english_sentences = open('data/de-en/europarl-v7.de-en.%s' % lang1, encoding='utf-8').read().strip().split('\\n')\n",
    "# german_sentences = open('data/de-en/europarl-v7.de-en.%s' % lang2, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "# Step 2: Tokenize the text using BERT tokenizer\n",
    "def tokenize_sentences(sentences, tokenizer):\n",
    "    return [tokenizer(sentence, return_tensors='pt', padding=True, truncation=True) for sentence in sentences]\n",
    "\n",
    "# Step 3: Embed the tokenized text using BERT model\n",
    "def embed_sentences(tokenized_sentences, model):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for tokens in tokenized_sentences:\n",
    "            outputs = model(**tokens)\n",
    "            embeddings.append(outputs.last_hidden_state)\n",
    "    return embeddings\n",
    "\n",
    "# Main function to process the corpus\n",
    "def process_parallel_corpus(english_file, german_file):\n",
    "    # Load data\n",
    "    english_sentences, german_sentences = load_data(english_file, german_file)\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "    \n",
    "    # Tokenize sentences\n",
    "    tokenized_english = tokenize_sentences(english_sentences, tokenizer)\n",
    "    tokenized_german = tokenize_sentences(german_sentences, tokenizer)\n",
    "    \n",
    "    # Embed sentences\n",
    "    english_embeddings = embed_sentences(tokenized_english, model)\n",
    "    german_embeddings = embed_sentences(tokenized_german, model)\n",
    "    \n",
    "    return english_embeddings, german_embeddings\n",
    "\n",
    "# Example usage\n",
    "english_file = 'data/de-en/europarl-v7.de-en.en'\n",
    "german_file = 'data/de-en/europarl-v7.de-en.de'\n",
    "# english_embeddings, german_embeddings = process_parallel_corpus(english_file, german_file)\n",
    "\n",
    "# english_embeddings and german_embeddings are now lists of tensors\n",
    "# print(english_embeddings[0].shape)  # Example: torch.Size([1, 10, 768]) for a sentence with 10 tokens\n",
    "# print(german_embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c9568a-b8a8-4972-9e7f-64364eb242c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b49ab2-8164-495a-bf2a-621b363d8534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00dcb7c-1e30-484a-811d-df5a9e794989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00476a-fe55-4358-992e-76c9611f10c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6d7ee-2f86-4fb9-9f7a-2626d40f668f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f009de0-4f7c-4617-a936-f54743bc91a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
