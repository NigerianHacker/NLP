{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e7877d",
   "metadata": {},
   "source": [
    "# Assignmnet 3 (100 + 5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b6d20",
   "metadata": {},
   "source": [
    "**Name: Alexander Vaptsarov** <br>\n",
    "**Email:** alv3651@thi.de<br>\n",
    "**Group:** A <br>\n",
    "**Hours spend *(optional)* : at least one week** <br>\n",
    "\n",
    "    \n",
    "    I appologize for my model its is not working I studied the 'Attention is all you need' paper and I understand the theorry behind attention and machine translation but I had problems with the dataset and also with the embedding so I did my best I rellly hope you can give me 62 pt. because that is how much I need to pass this practical\n",
    "\n",
    "    I tried explaing as much as I in this source code, Itryed a lot of different combinations but nothing is working. I will be improving this model in the future!\n",
    "    \n",
    "    I studied for this exam a lot so I would be very sad if I dont pass :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f79f88",
   "metadata": {},
   "source": [
    "### Question 1: Transformer model *(100 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd0c1a",
   "metadata": {},
   "source": [
    "As a Machine Learning engineer at a tech company, you were given a task to develop a machine translation system that translates **English (source) to German (Target)**. You can use existing libraries but the training needs to be done from scratch (usage of pretrained weights is not allowed). You have the freedom to select any dataset for training the model. Use a small subset of data as a validation dataset and report the BLEU score on the validation set. Also, provide a short description of your transformer model architecture, hyperparameters, and training (also provide the training loss curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a6e95c",
   "metadata": {},
   "source": [
    "<h3> Submission </h3>\n",
    "\n",
    "The test set **(test.txt)** will be released one week before the deadline. You should submit the output of your model on the test set separately. Name the output file as **\"first name_last_name_test_result.txt\"**. Each line of the submission file should contain only the translated text of the corresponding sentence from 'test.txt'.\n",
    "\n",
    "The 'first name_last_name_test_result.txt' file will be evaluated by your instructor and the student who could get the best BLEU score will get 5 additional points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4d2da",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "Here are some of the parallel datasets (see Datasets and Resources file):\n",
    "* Europarl Parallel corpus - https://www.statmt.org/europarl/v7/de-en.tgz\n",
    "* News Commentary - https://www.statmt.org/wmt14/training-parallel-nc-v9.tgz (use DE-EN parallel data)\n",
    "* Common Crawl corpus - https://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz (use DE-EN parallel data)\n",
    "\n",
    "You can also use other datasets of your choice. In the above datasets, **'.en'** file has the text in English, and **'.de'** file contains their corresponding German translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f05905b",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "1) You can also consider using a small subset of the dataset if the training dataset is large\n",
    "2) Sometimes you can also get out of memory errors while training, so choose the hyperparameters carefully.\n",
    "3) Your training will be much faster if you use a GPU. If you are using a CPU, it may take several hours or even days. (you can also use Google Colab GPUsÂ for training. link: https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12f5ab6e-8a9c-4cea-ab1f-4c59681d0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import copy\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, RandomSampler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# nltk.download('punkt')\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da2ebeb6-e60d-4016-9daf-848b2b56e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab, d_model):\n",
    "        super(Embedding, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.lut = nn.Embedding(vocab, d_model) # look up table \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f77650f-b30f-457c-be87-380d9fbdfd78",
   "metadata": {},
   "source": [
    "<img src='data/img/multi-head-attention_l1A3G7a.png' style='width: 300px'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "270a2b39-b832-4b4a-8f2f-79578baea274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        # the input consists of queries and keys of dim d_k (head dimension)\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        'linear projection (transformation)'\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False) \n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def scaled_dot_product(self, Q, K, V, mask=None):\n",
    "        'Scaled Dot-Product Attention'\n",
    "        d_k = Q.size()[-1]\n",
    "        score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) # scaled multiplcative compatibility function \n",
    "\n",
    "        attention = torch.softmax(score, dim=-1) # probability of attention \n",
    "        \n",
    "        return torch.matmul(attention, V)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        'This method splits the input tensor into multiple heads for parallel processing'\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        'After attention computation, the heads are combined back'\n",
    "        batch_size, _, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "                \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attention = self.scaled_dot_product(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attention))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80a26a34-eae4-459a-93d7-b8f5f5461448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W_q = nn.Linear(512, 512)\n",
    "# Q = torch.randint(low=1, high=1000, size=(8, 100, 512)).float()\n",
    "# x = W_q(Q)\n",
    "# x.size()\n",
    "# Q.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40e728bb-40fe-47c1-9997-8f398a0c291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a2a5c07-5842-4bbc-a842-7047c2971095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0a930-214c-4190-9425-566e48cef974",
   "metadata": {},
   "source": [
    "<img src='data/img/0*bPKV4ekQr9ZjYkWJ.webp' style='width: 300px'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2548e0a1-53ec-4d8f-9659-b1d99c0b4fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d81cd-dfd2-4885-826c-cfd2b6229920",
   "metadata": {},
   "source": [
    "<img src='data/img/0*SPZgT4k8GQi37H__.webp' style='width: 300px'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4189e32d-a8cc-4022-8a52-d55b45cf4e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_out = self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d3eee18-d7d9-426b-9770-f2a5b1cc806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([Encoder(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([Decoder(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac1f1d32-094c-405f-949d-fd0e80c229ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_model = 128\n",
    "# src_vocab = 1000\n",
    "# trg_vocab = 2000\n",
    "\n",
    "# src_pos = torch.randint(low=0, high=src_vocab, size=(2, 30))\n",
    "# trg_pos = torch.randint(low=0, high=trg_vocab, size=(2, 35))\n",
    "\n",
    "# model = Transformer(src_vocab, trg_vocab, d_model, num_enc_layer=2, num_dec_layer=2, n_heads=2, dropout=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d8934-3b16-4d6f-a773-eab9280a0987",
   "metadata": {},
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca143e0-2780-4193-87f0-f6ae45153d72",
   "metadata": {},
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8124e5-2c4d-45e4-b0db-3b456f9d3355",
   "metadata": {},
   "source": [
    "### Data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "051b71a2-9a97-45f5-ad2e-8b48ddb712b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a72799-21a9-45b8-be8b-36bdf6142309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(english_file, german_file):\n",
    "    with open(english_file, 'r', encoding='utf-8') as f:\n",
    "        english_sentences = f.readlines()\n",
    "    with open(german_file, 'r', encoding='utf-8') as f:\n",
    "        german_sentences = f.readlines()\n",
    "    return english_sentences, german_sentences\n",
    "    \n",
    "english_file = 'data/de-en/europarl-v7.de-en.en'\n",
    "german_file = 'data/de-en/europarl-v7.de-en.de'\n",
    "src, trg = load_data(english_file, german_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced3bbd-7c8f-4946-8b46-bc97485f8267",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04eff65-05a0-4c9b-9b2b-7bc3d8badd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train(model, data_loader, optimizer, criterion, num_epochs=10):\n",
    "    model.train() \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for src, tgt in data_loader:\n",
    "            src = src.to(device)\n",
    "            tgt_input = tgt[:, :-1].to(device)\n",
    "            tgt_output = tgt[:, 1:].to(device)\n",
    "\n",
    "            def generate_mask(src, tgt):\n",
    "                src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "                tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "                seq_length = tgt.size(1)\n",
    "                nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "                tgt_mask = tgt_mask & nopeak_mask\n",
    "                return src_mask, tgt_mask\n",
    "                \n",
    "            # Create masks and padding masks for the src and target\n",
    "            src_mask, tgt_mask = generate_mask(src.size(1), tgt_input.size(1)).to(device)\n",
    "            src_padding_mask = (src == 0).transpose(0, 1).to(device)\n",
    "            tgt_padding_mask = (tgt_input == 0).transpose(0, 1).to(device)\n",
    "            memory_key_padding_mask = src_padding_mask.clone()\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            output = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(data_loader)}')\n",
    "\n",
    "batch_size = 64\n",
    "src_vocab_size = 10000 \n",
    "tgt_vocab_size = 10000  \n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layer = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming '0' is the padding index\n",
    "\n",
    "# Dummy data for demonstration: actual data should be tensors of token indices\n",
    "# These should be replaced with actual data loading part\n",
    "src_data = torch.randint(1, src_vocab_size, (1000, 35))  # (batch_size, sequence_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (1000, 36))  # (batch_size, sequence_length)\n",
    "dataset = TensorDataset(src_data, tgt_data)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "num_epochs = 10  # Number of epochs to train\n",
    "train(model, data_loader, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d671919-fd60-47a9-bb8c-66da368fc004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d342c0-1480-44f6-ad4f-73eba3a463f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=2):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter(chain(*sentence_list))\n",
    "        self.itos.update({word: idx for idx, (word, freq) in enumerate(frequencies.items(), len(self.itos)) if freq >= self.freq_threshold})\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in text]\n",
    "\n",
    "def tokenize_english(text):\n",
    "    return [tok.lower() for tok in text.split()]\n",
    "\n",
    "def tokenize_french(text):\n",
    "    return [tok.lower() for tok in text.split()]\n",
    "\n",
    "class ParallelDataset(Dataset):\n",
    "    def __init__(self, data, src_vocab, tgt_vocab):\n",
    "        self.data = data\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_sentence, tgt_sentence = self.data[idx].split(\"\\t\")\n",
    "        src = [self.src_vocab.stoi[\"<SOS>\"]] + self.src_vocab.numericalize(tokenize_french(src_sentence)) + [self.src_vocab.stoi[\"<EOS>\"]]\n",
    "        tgt = [self.tgt_vocab.stoi[\"<SOS>\"]] + self.tgt_vocab.numericalize(tokenize_english(tgt_sentence)) + [self.tgt_vocab.stoi[\"<EOS>\"]]\n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "\n",
    "# Build vocabularies\n",
    "src_vocab = Vocabulary()\n",
    "tgt_vocab = Vocabulary()\n",
    "src_vocab.build_vocabulary([tokenize_french(sentence.split(\"\\t\")[0]) for sentence in parallel_data])\n",
    "tgt_vocab.build_vocabulary([tokenize_english(sentence.split(\"\\t\")[1]) for sentence in parallel_data])\n",
    "\n",
    "# Create dataset\n",
    "dataset = ParallelDataset(parallel_data, src_vocab, tgt_vocab)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# For demonstration, let's print one batch\n",
    "for src, tgt in data_loader:\n",
    "    print(\"Source Batch:\", src)\n",
    "    print(\"Target Batch:\", tgt)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e855b6d-b61f-4dd1-b724-758212c97922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc59051-cebd-4470-9c8e-ec9778d4a8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a960f1f-8140-4eea-9a38-62c11fb88559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc287f8b-74fd-4f73-825f-891976de2644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
